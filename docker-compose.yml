name: ollama
version: "3.8"

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    pull_policy: always
    restart: unless-stopped
    # Uncomment below for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities:
    #             - gpu

    # Uncomment below to expose Ollama API outside the container stack
    ports:
      - 11435:11434
    volumes:
      - ollama_data:/root/.ollama
    # tty: true

  kong:
    image: kong:3.7.1-ubuntu
    container_name: ollama_kong
    restart: always
    environment:
      - KONG_DATABASE=off
      - KONG_DECLARATIVE_CONFIG=/etc/kong/conf/kong.yml
      - KONG_ADMIN_LISTEN=0.0.0.0:8001
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_HEADERS=off
    volumes:
      - ./conf:/etc/kong/conf
    ports:
      - 5050:8000 # Kong Proxy HTTP port
      - 127.0.0.1:5051:8001 # Kong Admin port

volumes:
  ollama_data:
